{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Assignment 4\n",
    "\n",
    "Previously in 2_fullyconnected.ipynb and 3_regularization.ipynb, we trained fully connected networks to classify notMNIST characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、格式化数据\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1  # grayscale\n",
    "\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape(\n",
    "        (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "NUM_LABELS = 10\n",
    "\n",
    "CONV1_DEEP = 32\n",
    "CONV1_SIZE = 5\n",
    "\n",
    "CONV2_DEEP = 64\n",
    "CONV2_SIZE = 5\n",
    "\n",
    "FC_SIZE = 512\n",
    "\n",
    "\n",
    "def inference(input_tensor, train, regularizer):\n",
    "    with tf.variable_scope('layer1-conv1'):\n",
    "        conv1_weights = tf.get_variable(\n",
    "            \"weight\", [CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP],\n",
    "            initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv1_biases = tf.get_variable(\n",
    "            \"bias\", [CONV1_DEEP], initializer=tf.constant_initializer(0.0))\n",
    "        conv1 = tf.nn.conv2d(input_tensor, conv1_weights,\n",
    "                             strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))\n",
    "\n",
    "    with tf.name_scope(\"layer2-pool1\"):\n",
    "        pool1 = tf.nn.max_pool(relu1, ksize=[1, 2, 2, 1], strides=[\n",
    "                               1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "    with tf.variable_scope(\"layer3-conv2\"):\n",
    "        conv2_weights = tf.get_variable(\n",
    "            \"weight\", [CONV2_SIZE, CONV2_SIZE, CONV1_DEEP, CONV2_DEEP],\n",
    "            initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv2_biases = tf.get_variable(\n",
    "            \"bias\", [CONV2_DEEP], initializer=tf.constant_initializer(0.0))\n",
    "        conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[\n",
    "                             1, 1, 1, 1], padding='SAME')\n",
    "        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n",
    "\n",
    "    with tf.name_scope(\"layer4-pool2\"):\n",
    "        pool2 = tf.nn.max_pool(relu2, ksize=[1, 2, 2, 1], strides=[\n",
    "                               1, 2, 2, 1], padding='SAME')\n",
    "        pool_shape = pool2.get_shape().as_list()\n",
    "        nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]\n",
    "        reshaped = tf.reshape(pool2, [pool_shape[0], nodes])\n",
    "\n",
    "    # 全连接层_1\n",
    "    with tf.variable_scope('layer5-fc1'):\n",
    "        fc1_weights = tf.get_variable(\"weight\", [nodes, FC_SIZE],\n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer != None:\n",
    "            tf.add_to_collection('losses', regularizer(fc1_weights))\n",
    "        fc1_biases = tf.get_variable(\n",
    "            \"bias\", [FC_SIZE], initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "        fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)\n",
    "        if train:\n",
    "            fc1 = tf.nn.dropout(fc1, 0.5) # 一般在全连接层使用dropout\n",
    "    \n",
    "    # 全连接层_2\n",
    "    with tf.variable_scope('layer6-fc2'):\n",
    "        fc2_weights = tf.get_variable(\"weight\", [FC_SIZE, NUM_LABELS],\n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer != None:\n",
    "            tf.add_to_collection('losses', regularizer(fc2_weights))\n",
    "        fc2_biases = tf.get_variable(\n",
    "            \"bias\", [NUM_LABELS], initializer=tf.constant_initializer(0.1))\n",
    "        logit = tf.matmul(fc1, fc2_weights) + fc2_biases\n",
    "\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、定义网络参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE_BASE = 0.01\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 10000\n",
    "MOVING_AVERAGE_DECAY = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、定义训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1 training step(s), loss on training batch is 6.6443.\n",
      "After 101 training step(s), loss on training batch is 1.67447.\n",
      "After 201 training step(s), loss on training batch is 1.67114.\n",
      "After 301 training step(s), loss on training batch is 1.64158.\n",
      "After 401 training step(s), loss on training batch is 1.52455.\n",
      "After 501 training step(s), loss on training batch is 1.3042.\n",
      "After 601 training step(s), loss on training batch is 1.32527.\n",
      "After 701 training step(s), loss on training batch is 1.29104.\n",
      "After 801 training step(s), loss on training batch is 1.76907.\n",
      "After 901 training step(s), loss on training batch is 1.37023.\n",
      "After 1001 training step(s), loss on training batch is 1.2363.\n",
      "After 1101 training step(s), loss on training batch is 1.38587.\n",
      "After 1201 training step(s), loss on training batch is 1.37042.\n",
      "After 1301 training step(s), loss on training batch is 1.50662.\n",
      "After 1401 training step(s), loss on training batch is 1.42075.\n",
      "After 1501 training step(s), loss on training batch is 1.01284.\n",
      "After 1601 training step(s), loss on training batch is 1.21666.\n",
      "After 1701 training step(s), loss on training batch is 1.25789.\n",
      "After 1801 training step(s), loss on training batch is 1.26719.\n",
      "After 1901 training step(s), loss on training batch is 1.27983.\n",
      "After 2001 training step(s), loss on training batch is 1.30381.\n",
      "After 2101 training step(s), loss on training batch is 1.24993.\n",
      "After 2201 training step(s), loss on training batch is 1.15294.\n",
      "After 2301 training step(s), loss on training batch is 1.20837.\n",
      "After 2401 training step(s), loss on training batch is 1.35816.\n",
      "After 2501 training step(s), loss on training batch is 1.22119.\n",
      "After 2601 training step(s), loss on training batch is 1.31475.\n",
      "After 2701 training step(s), loss on training batch is 0.967491.\n",
      "After 2801 training step(s), loss on training batch is 1.04552.\n",
      "After 2901 training step(s), loss on training batch is 1.24977.\n",
      "After 3001 training step(s), loss on training batch is 1.17412.\n",
      "After 3101 training step(s), loss on training batch is 1.15808.\n",
      "After 3201 training step(s), loss on training batch is 1.30904.\n",
      "After 3301 training step(s), loss on training batch is 0.914351.\n",
      "After 3401 training step(s), loss on training batch is 1.32877.\n",
      "After 3501 training step(s), loss on training batch is 1.02774.\n",
      "After 3601 training step(s), loss on training batch is 1.2779.\n",
      "After 3701 training step(s), loss on training batch is 0.996221.\n",
      "After 3801 training step(s), loss on training batch is 1.08477.\n",
      "After 3901 training step(s), loss on training batch is 1.18308.\n",
      "After 4001 training step(s), loss on training batch is 1.20096.\n",
      "After 4101 training step(s), loss on training batch is 1.06747.\n",
      "After 4201 training step(s), loss on training batch is 0.933304.\n",
      "After 4301 training step(s), loss on training batch is 1.19737.\n",
      "After 4401 training step(s), loss on training batch is 1.11656.\n",
      "After 4501 training step(s), loss on training batch is 1.13988.\n",
      "After 4601 training step(s), loss on training batch is 1.0735.\n",
      "After 4701 training step(s), loss on training batch is 1.30286.\n",
      "After 4801 training step(s), loss on training batch is 0.946141.\n",
      "After 4901 training step(s), loss on training batch is 1.01376.\n",
      "After 5001 training step(s), loss on training batch is 1.25398.\n",
      "After 5101 training step(s), loss on training batch is 1.16871.\n",
      "After 5201 training step(s), loss on training batch is 1.18334.\n",
      "After 5301 training step(s), loss on training batch is 1.11658.\n",
      "After 5401 training step(s), loss on training batch is 1.04113.\n",
      "After 5501 training step(s), loss on training batch is 0.96844.\n",
      "After 5601 training step(s), loss on training batch is 0.912337.\n",
      "After 5701 training step(s), loss on training batch is 1.13013.\n",
      "After 5801 training step(s), loss on training batch is 1.00395.\n",
      "After 5901 training step(s), loss on training batch is 0.967875.\n",
      "After 6001 training step(s), loss on training batch is 0.94047.\n",
      "After 6101 training step(s), loss on training batch is 0.942563.\n",
      "After 6201 training step(s), loss on training batch is 0.887759.\n",
      "After 6301 training step(s), loss on training batch is 1.05962.\n",
      "After 6401 training step(s), loss on training batch is 1.02067.\n",
      "After 6501 training step(s), loss on training batch is 1.2013.\n",
      "After 6601 training step(s), loss on training batch is 0.944508.\n",
      "After 6701 training step(s), loss on training batch is 1.04839.\n",
      "After 6801 training step(s), loss on training batch is 1.12988.\n",
      "After 6901 training step(s), loss on training batch is 1.09104.\n",
      "After 7001 training step(s), loss on training batch is 1.21299.\n",
      "After 7101 training step(s), loss on training batch is 1.01962.\n",
      "After 7201 training step(s), loss on training batch is 0.982476.\n",
      "After 7301 training step(s), loss on training batch is 1.16214.\n",
      "After 7401 training step(s), loss on training batch is 1.46415.\n",
      "After 7501 training step(s), loss on training batch is 1.16816.\n",
      "After 7601 training step(s), loss on training batch is 1.06875.\n",
      "After 7701 training step(s), loss on training batch is 0.888094.\n",
      "After 7801 training step(s), loss on training batch is 1.07696.\n",
      "After 7901 training step(s), loss on training batch is 1.11113.\n",
      "After 8001 training step(s), loss on training batch is 1.16584.\n",
      "After 8101 training step(s), loss on training batch is 0.875224.\n",
      "After 8201 training step(s), loss on training batch is 0.947531.\n",
      "After 8301 training step(s), loss on training batch is 0.909526.\n",
      "After 8401 training step(s), loss on training batch is 0.828179.\n",
      "After 8501 training step(s), loss on training batch is 1.10713.\n",
      "After 8601 training step(s), loss on training batch is 1.16586.\n",
      "After 8701 training step(s), loss on training batch is 0.929636.\n",
      "After 8801 training step(s), loss on training batch is 1.05452.\n",
      "After 8901 training step(s), loss on training batch is 1.172.\n",
      "After 9001 training step(s), loss on training batch is 1.04543.\n",
      "After 9101 training step(s), loss on training batch is 1.07173.\n",
      "After 9201 training step(s), loss on training batch is 0.956362.\n",
      "After 9301 training step(s), loss on training batch is 0.986565.\n",
      "After 9401 training step(s), loss on training batch is 0.952067.\n",
      "After 9501 training step(s), loss on training batch is 0.814326.\n",
      "After 9601 training step(s), loss on training batch is 0.986405.\n",
      "After 9701 training step(s), loss on training batch is 0.918967.\n",
      "After 9801 training step(s), loss on training batch is 1.07477.\n",
      "After 9901 training step(s), loss on training batch is 1.10275.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def train():\n",
    "    # 定义输出为4维矩阵的placeholder\n",
    "    x = tf.placeholder(tf.float32, [\n",
    "            BATCH_SIZE,\n",
    "            IMAGE_SIZE,\n",
    "            IMAGE_SIZE,\n",
    "            NUM_CHANNELS],\n",
    "        name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-input')\n",
    "    \n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    y = inference(x,True,regularizer)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # 定义损失函数、学习率、滑动平均操作以及训练过程。\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        train_dataset.shape[0]/ BATCH_SIZE, LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "    \n",
    "    # 模型评估\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "    # 初始化TensorFlow持久化类。\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            offset = (i * BATCH_SIZE) % (train_labels.shape[0] - BATCH_SIZE)\n",
    "            xs = train_dataset[offset:(offset + BATCH_SIZE), :]\n",
    "            ys = train_labels[offset:(offset + BATCH_SIZE), :]\n",
    "\n",
    "            reshaped_xs = np.reshape(xs, (\n",
    "                BATCH_SIZE,\n",
    "                IMAGE_SIZE,\n",
    "                IMAGE_SIZE,\n",
    "                NUM_CHANNELS))\n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: reshaped_xs, y_: ys})\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(\"After %d training step(s), loss on training batch is %g.\" % (step, loss_value))\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
