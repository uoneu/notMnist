{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "##  Assignment 3\n",
    "Previously in 2_fullyconnected.ipynb, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n",
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  加入正则化、学习率指数衰减 、dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 2.300287\n",
      "Minibatch accuracy: 0.1\n",
      "Validation accuracy: 0.100000\n",
      "Minibatch loss at step 500: 0.682174\n",
      "Minibatch accuracy: 0.8\n",
      "Validation accuracy: 0.850800\n",
      "Minibatch loss at step 1000: 0.481413\n",
      "Minibatch accuracy: 0.9\n",
      "Validation accuracy: 0.855100\n",
      "Minibatch loss at step 1500: 0.543780\n",
      "Minibatch accuracy: 0.8\n",
      "Validation accuracy: 0.870800\n",
      "Minibatch loss at step 2000: 0.491746\n",
      "Minibatch accuracy: 0.8\n",
      "Validation accuracy: 0.875200\n",
      "Minibatch loss at step 2500: 0.535162\n",
      "Minibatch accuracy: 0.9\n",
      "Validation accuracy: 0.874500\n",
      "Minibatch loss at step 3000: 0.475902\n",
      "Minibatch accuracy: 0.9\n",
      "Validation accuracy: 0.880900\n",
      "Minibatch loss at step 3500: 0.441360\n",
      "Minibatch accuracy: 0.9\n",
      "Validation accuracy: 0.887900\n",
      "Minibatch loss at step 4000: 0.504814\n",
      "Minibatch accuracy: 0.9\n",
      "Validation accuracy: 0.888300\n",
      "Minibatch loss at step 4500: 0.360647\n",
      "Minibatch accuracy: 0.9\n",
      "Validation accuracy: 0.886800\n",
      "Minibatch loss at step 5000: 0.352049\n",
      "Minibatch accuracy: 0.9\n",
      "Validation accuracy: 0.889900\n",
      "Test accuracy: 0.943500\n"
     ]
    }
   ],
   "source": [
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "LAYER1_NODE = 1024\n",
    "LAYER2_NODE = 256\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "\n",
    "\n",
    "REGULATION_RATE = 5e-4\n",
    "TRAINING_STEP = 5001\n",
    "MOVING_AVERAG_EATE = 0.99\n",
    "\n",
    "# train_dataset = train_dataset[0:BATCH_SIZE*2]\n",
    "# train_labels = train_labels[0:BATCH_SIZE*2]\n",
    "\n",
    "def inference(x, avg_class, w1, b1, w2, b2):\n",
    "    if avg_class == None:\n",
    "        layer1 = tf.nn.relu(tf.matmul(x, w1)+b1)\n",
    "        return tf.matmul(layer1, w2) + b2\n",
    "    else:\n",
    "        ''' 使用滑动平均中的影子变量'''\n",
    "        layer1 = tf.nn.relu(tf.matmul(x, avg_class.average(w1))+avg_class.average(b1))\n",
    "        return tf.matmul(layer1, avg_class.average(w2))+avg_class.average(b2)\n",
    "\n",
    "\n",
    "def train():\n",
    "    # 定义三层神经网络\n",
    "    x = tf.placeholder(tf.float32, shape=[None, INPUT_NODE])\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, OUTPUT_NODE])\n",
    "    \n",
    "    hidden_w1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE],\\\n",
    "                                                stddev=2.0/math.sqrt(INPUT_NODE*LAYER1_NODE)))\n",
    "    hidden_b1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))\n",
    "    \n",
    "    layer1 = tf.nn.relu(tf.matmul(x, hidden_w1)+hidden_b1)\n",
    "        \n",
    "    hidden_w2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, LAYER2_NODE],\\\n",
    "                                                stddev=2.0/math.sqrt(LAYER1_NODE*LAYER2_NODE)))\n",
    "    hidden_b2 = tf.Variable(tf.constant(0.1, shape=[LAYER2_NODE]))\n",
    "    \n",
    "    layer2 = tf.nn.relu(tf.matmul(layer1, hidden_w2)+hidden_b2)\n",
    "        \n",
    "    hidden_w3 = tf.Variable(tf.truncated_normal([LAYER2_NODE, OUTPUT_NODE],\\\n",
    "                                                stddev=2.0/math.sqrt(LAYER2_NODE*OUTPUT_NODE)))\n",
    "    hidden_b3 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n",
    "      \n",
    "    y = tf.matmul(layer2, hidden_w3) + hidden_b3\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # 交叉熵\n",
    "    cross_entroy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.argmax(y_,1), logits=y)\n",
    "    cross_entroy_mean = tf.reduce_mean(cross_entroy)\n",
    "\n",
    "    # 加入正则化\n",
    "    regulater = tf.contrib.layers.l2_regularizer(REGULATION_RATE)\n",
    "    regulation = regulater(hidden_w1) + regulater(hidden_w2) + regulater(hidden_w3)\n",
    "\n",
    "    # 学习率指数衰减\n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step,\\\n",
    "                            train_dataset.shape[0]/BATCH_SIZE, LEARNING_RATE_DECAY)\n",
    "\n",
    "    # 定义损失 & 优化算法\n",
    "    loss = cross_entroy_mean + regulation\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # 模型评估\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "  \n",
    "    # 训练神经网络\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        # 验证数据\n",
    "        validate_feed = { x : valid_dataset, y_ : valid_labels}\n",
    "        # 测试数据 \n",
    "        test_feed = { x : test_dataset, y_ : test_labels}\n",
    "\n",
    "        # 迭代训练更新参数\n",
    "        for step in range(TRAINING_STEP):\n",
    "            offset = (step * BATCH_SIZE) % (train_labels.shape[0] - BATCH_SIZE)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + BATCH_SIZE), :]\n",
    "            batch_labels = train_labels[offset:(offset + BATCH_SIZE), :]\n",
    "            train_feed = { x: batch_data, y_: batch_labels }\n",
    "            _, l, train_accuracy = sess.run( [optimizer, loss, accuracy], feed_dict=train_feed)\n",
    "            if (step % 500 == 0):\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.1f\" %train_accuracy)\n",
    "                valid_accuracy = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                print(\"Validation accuracy: %f\" % valid_accuracy)\n",
    "        test_accuracy = sess.run(accuracy, feed_dict=test_feed)\n",
    "        print(\"Test accuracy: %f\" %test_accuracy)\n",
    "\n",
    "        \n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
